class VeryTinyNeRFMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.L_pos = 6
        self.L_dir = 4
        pos_enc_feats = 3 + 3 * 2 * self.L_pos
        dir_enc_feats = 3 + 3 * 2 * self.L_dir
        net_width = 256
        self.early_mlp = nn.Sequential(
            nn.Linear(pos_enc_feats, net_width),
            nn.ReLU(),
            nn.Linear(net_width, net_width + 1),
            nn.ReLU(),
        )
        self.middle_mlp1 = nn.Sequential(
            nn.Linear(net_width + dir_enc_feats, net_width//2),
            nn.ReLU(),
            nn.Linear(net_width//2, net_width//4 + 1),
            nn.ReLU(),
        )
        self.middle_mlp2 = nn.Sequential(
            nn.Linear(net_width//4 + dir_enc_feats, net_width//4 + 1),
            nn.ReLU(),
            nn.Linear(net_width//4 + 1, net_width//2 + 1),
            nn.ReLU(),
        )
        self.late_mlp = nn.Sequential(
            nn.Linear(net_width//2 + dir_enc_feats, net_width),
            nn.ReLU(),
            nn.Linear(net_width, 3),
            nn.Sigmoid(),
        )
    def forward(self, xs, ds):
        xs_encoded = [xs]
        for l_pos in range(self.L_pos):
            xs_encoded.append(torch.sin(2**l_pos * torch.pi * xs))
            xs_encoded.append(torch.cos(2**l_pos * torch.pi * xs))
        xs_encoded = torch.cat(xs_encoded, dim=-1)
        ds = ds / ds.norm(p=2, dim=-1).unsqueeze(-1)
        ds_encoded = [ds]
        for l_dir in range(self.L_dir):
            ds_encoded.append(torch.sin(2**l_dir * torch.pi * ds))
            ds_encoded.append(torch.cos(2**l_dir * torch.pi * ds))
        ds_encoded = torch.cat(ds_encoded, dim=-1)

        outputs = self.early_mlp(xs_encoded)

        inp_middle_mlp = torch.cat([outputs[:, 1:], ds_encoded], dim=-1)
        outputs = self.middle_mlp1(inp_middle_mlp)
        sigma_is = outputs[:, 0]

        inp_middle_mlp = torch.cat([outputs[:, 1:], ds_encoded], dim=-1)
        outputs = self.middle_mlp2(inp_middle_mlp)
        sigma_is = outputs[:, 0]

        c_is = self.late_mlp(torch.cat([outputs[:, 1:], ds_encoded], dim=-1))

        return {"c_is": c_is, "sigma_is": sigma_is}
